{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e099cd-bb73-45de-a1b3-47f3e9bba3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"Agg\")\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    fbeta_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "\n",
    "# Use a fixed business snapshot date for both CLV and churn labeling\n",
    "SNAPSHOT_DATE = pd.Timestamp(\"2025-06-30\")\n",
    "\n",
    "RESULTS_DIR = \"saved_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Adjust DATA_PATH if you are not on Kaggle\n",
    "DATA_PATH = \"/kaggle/input/netflix-2025user-behavior-dataset-210k-records\"\n",
    "print(\"Using data path:\", DATA_PATH)\n",
    "print(\"Saving results to:\", os.path.abspath(RESULTS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85279435-bcbb-4454-97cf-52c91528f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_history = pd.read_csv(os.path.join(DATA_PATH, \"watch_history.csv\"))\n",
    "users          = pd.read_csv(os.path.join(DATA_PATH, \"users.csv\"))\n",
    "reviews        = pd.read_csv(os.path.join(DATA_PATH, \"reviews.csv\"))\n",
    "recommendation_logs = pd.read_csv(os.path.join(DATA_PATH, \"recommendation_logs.csv\"))\n",
    "movies         = pd.read_csv(os.path.join(DATA_PATH, \"movies.csv\"))\n",
    "search_logs    = pd.read_csv(os.path.join(DATA_PATH, \"search_logs.csv\"))\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"users:\", users.shape)\n",
    "print(\"watch_history:\", watch_history.shape)\n",
    "print(\"reviews:\", reviews.shape)\n",
    "print(\"recommendation_logs:\", recommendation_logs.shape)\n",
    "print(\"movies:\", movies.shape)\n",
    "print(\"search_logs:\", search_logs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b016b09-399b-4619-be62-3c8855e30be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns\n",
    "watch_history[\"watch_date\"] = pd.to_datetime(watch_history[\"watch_date\"], errors=\"coerce\")\n",
    "\n",
    "users[\"subscription_start_date\"] = pd.to_datetime(users[\"subscription_start_date\"], errors=\"coerce\")\n",
    "users[\"created_at\"] = pd.to_datetime(users[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "reviews[\"review_date\"] = pd.to_datetime(reviews[\"review_date\"], errors=\"coerce\")\n",
    "recommendation_logs[\"recommendation_date\"] = pd.to_datetime(recommendation_logs[\"recommendation_date\"], errors=\"coerce\")\n",
    "search_logs[\"search_date\"] = pd.to_datetime(search_logs[\"search_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"added_to_platform\" in movies.columns:\n",
    "    movies[\"added_to_platform\"] = pd.to_datetime(movies[\"added_to_platform\"], errors=\"coerce\")\n",
    "\n",
    "datasets = {\n",
    "    \"users\": users,\n",
    "    \"watch_history\": watch_history,\n",
    "    \"reviews\": reviews,\n",
    "    \"recommendation_logs\": recommendation_logs,\n",
    "    \"movies\": movies,\n",
    "    \"search_logs\": search_logs,\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"Missing percentage in {name}:\")\n",
    "    print((df.isnull().mean() * 100).round(2))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e437944-412b-440e-b422-bbd758f336a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_flag(df, columns, value):\n",
    "    \"\"\"Impute selected columns and add *_was_missing flags.\"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        flag_col = f\"{col}_was_missing\"\n",
    "        df[flag_col] = df[col].isna().astype(int)\n",
    "        df[col] = df[col].fillna(value)\n",
    "\n",
    "# Users imputations\n",
    "fill_with_flag(users, [\"age\", \"gender\", \"monthly_spend\", \"household_size\"], np.nan)\n",
    "\n",
    "users[\"age\"] = users.groupby(\"gender\")[\"age\"].transform(lambda x: x.fillna(x.median()))\n",
    "users[\"age\"].fillna(users[\"age\"].median(), inplace=True)\n",
    "users[\"gender\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "users[\"monthly_spend\"] = users.groupby(\"subscription_plan\")[\"monthly_spend\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "users[\"monthly_spend\"].fillna(users[\"monthly_spend\"].median(), inplace=True)\n",
    "\n",
    "users[\"household_size\"].fillna(users[\"household_size\"].median(), inplace=True)\n",
    "users[\"primary_device\"].fillna(\"Unknown\", inplace=True)\n",
    "users[\"country\"].fillna(\"Unknown\", inplace=True)\n",
    "users[\"state_province\"].fillna(\"Unknown\", inplace=True)\n",
    "users[\"city\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Watch history imputations\n",
    "fill_with_flag(watch_history, [\"watch_duration_minutes\", \"progress_percentage\", \"user_rating\"], np.nan)\n",
    "\n",
    "if \"watch_duration_minutes\" not in watch_history.columns and \"watch_duration\" in watch_history.columns:\n",
    "    watch_history[\"watch_duration_minutes\"] = watch_history[\"watch_duration\"]\n",
    "watch_history[\"watch_duration_minutes\"] = pd.to_numeric(\n",
    "    watch_history[\"watch_duration_minutes\"], errors=\"coerce\"\n",
    ")\n",
    "watch_history[\"watch_duration_minutes\"] = watch_history.groupby(\"movie_id\")[\n",
    "    \"watch_duration_minutes\"\n",
    "].transform(lambda x: x.fillna(x.median()))\n",
    "watch_history[\"watch_duration_minutes\"].fillna(\n",
    "    watch_history[\"watch_duration_minutes\"].median(), inplace=True\n",
    ")\n",
    "\n",
    "watch_history[\"progress_percentage\"] = pd.to_numeric(\n",
    "    watch_history[\"progress_percentage\"], errors=\"coerce\"\n",
    ")\n",
    "watch_history[\"progress_percentage\"].fillna(watch_history[\"progress_percentage\"].median(), inplace=True)\n",
    "\n",
    "if \"user_rating\" in watch_history.columns:\n",
    "    watch_history[\"user_rating\"].fillna(-1, inplace=True)\n",
    "\n",
    "if \"is_download\" in watch_history.columns:\n",
    "    watch_history[\"is_download\"] = watch_history[\"is_download\"].astype(int)\n",
    "else:\n",
    "    watch_history[\"is_download\"] = 0\n",
    "\n",
    "# Reviews table\n",
    "fill_with_flag(reviews, [\"helpful_votes\", \"total_votes\", \"review_text\", \"sentiment_score\"], np.nan)\n",
    "reviews[\"helpful_votes\"].fillna(0, inplace=True)\n",
    "reviews[\"total_votes\"].fillna(0, inplace=True)\n",
    "reviews[\"review_text\"].fillna(\"\", inplace=True)\n",
    "reviews[\"sentiment_score\"].fillna(0, inplace=True)\n",
    "\n",
    "# Recommendation logs\n",
    "fill_with_flag(recommendation_logs, [\"recommendation_score\", \"algorithm_version\"], np.nan)\n",
    "recommendation_logs[\"recommendation_score\"] = recommendation_logs.groupby(\"recommendation_type\")[\n",
    "    \"recommendation_score\"\n",
    "].transform(lambda x: x.fillna(x.median()))\n",
    "recommendation_logs[\"recommendation_score\"].fillna(\n",
    "    recommendation_logs[\"recommendation_score\"].median(), inplace=True\n",
    ")\n",
    "recommendation_logs[\"algorithm_version\"].fillna(\"Unknown\", inplace=True)\n",
    "if \"was_clicked\" in recommendation_logs.columns:\n",
    "    recommendation_logs[\"was_clicked\"] = recommendation_logs[\"was_clicked\"].astype(int)\n",
    "else:\n",
    "    recommendation_logs[\"was_clicked\"] = 0\n",
    "\n",
    "# Movies table\n",
    "numeric_cols_movies = movies.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols_movies:\n",
    "    movies[col].fillna(movies[col].median(), inplace=True)\n",
    "\n",
    "object_cols_movies = movies.select_dtypes(include=\"object\").columns\n",
    "for col in object_cols_movies:\n",
    "    movies[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Search logs\n",
    "search_logs.fillna({\"search_query\": \"Unknown\"}, inplace=True)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name} remaining missing values:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf86f46-bfa9-48ad-88d9-f5fab68eae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names if any variants exist\n",
    "users.rename(columns={\"Userid\": \"user_id\"}, inplace=True)\n",
    "watch_history.rename(columns={\"Userid\": \"user_id\", \"MovieID\": \"movie_id\"}, inplace=True)\n",
    "reviews.rename(columns={\"Userid\": \"user_id\", \"MovieID\": \"movie_id\"}, inplace=True)\n",
    "recommendation_logs.rename(columns={\"Userid\": \"user_id\"}, inplace=True)\n",
    "search_logs.rename(columns={\"Userid\": \"user_id\"}, inplace=True)\n",
    "movies.rename(columns={\"MovieID\": \"movie_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffc467-cf80-4024-bac0-08c9075012f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Base financial layer (users)\n",
    "# -----------------------------\n",
    "user_features = users.copy()\n",
    "\n",
    "plan_map = {\"Basic\": 1, \"Standard\": 2, \"Premium\": 3, \"Premium+\": 4}\n",
    "user_features[\"tenure_days\"] = (SNAPSHOT_DATE - user_features[\"subscription_start_date\"]).dt.days.clip(lower=0)\n",
    "user_features[\"tenure_days\"].fillna(0, inplace=True)\n",
    "user_features[\"months_active\"] = (user_features[\"tenure_days\"] / 30.0).clip(lower=0)\n",
    "user_features[\"plan_tier\"] = user_features[\"subscription_plan\"].map(plan_map).fillna(0)\n",
    "user_features[\"monthly_spend\"].fillna(0, inplace=True)\n",
    "\n",
    "user_features[\"cumulative_revenue\"] = user_features[\"monthly_spend\"] * user_features[\"months_active\"]\n",
    "user_features[\"clv_baseline\"] = user_features[\"cumulative_revenue\"].clip(lower=0)\n",
    "\n",
    "if \"promotion_flag\" not in user_features.columns:\n",
    "    user_features[\"promotion_flag\"] = 0\n",
    "\n",
    "# -----------------------------\n",
    "# Engagement metrics from watch_history\n",
    "# -----------------------------\n",
    "watch_agg = watch_history.groupby(\"user_id\").agg(\n",
    "    total_watch_minutes=(\"watch_duration_minutes\", \"sum\"),\n",
    "    avg_session_minutes=(\"watch_duration_minutes\", \"mean\"),\n",
    "    completion_ratio=(\"progress_percentage\", \"mean\"),\n",
    "    device_diversity=(\"device_type\", pd.Series.nunique),\n",
    "    downloads_ratio=(\"is_download\", \"mean\"),\n",
    "    total_sessions=(\"session_id\", pd.Series.nunique),\n",
    "    first_watch_date=(\"watch_date\", \"min\"),\n",
    "    last_watch_date=(\"watch_date\", \"max\"),\n",
    ").reset_index()\n",
    "\n",
    "for col in [\n",
    "    \"total_watch_minutes\",\n",
    "    \"avg_session_minutes\",\n",
    "    \"completion_ratio\",\n",
    "    \"device_diversity\",\n",
    "    \"downloads_ratio\",\n",
    "    \"total_sessions\",\n",
    "]:\n",
    "    watch_agg[col].fillna(0, inplace=True)\n",
    "\n",
    "watch_agg[\"total_watch_hours\"] = watch_agg[\"total_watch_minutes\"] / 60.0\n",
    "watch_agg[\"engagement_span_days\"] = (\n",
    "    watch_agg[\"last_watch_date\"] - watch_agg[\"first_watch_date\"]\n",
    ").dt.days.fillna(0)\n",
    "\n",
    "# Recency within historical window (no leakage; use SNAPSHOT_DATE)\n",
    "watch_agg[\"recency_days\"] = (SNAPSHOT_DATE - watch_agg[\"last_watch_date\"]).dt.days\n",
    "\n",
    "# -----------------------------\n",
    "# Search engagement\n",
    "# -----------------------------\n",
    "search_agg = search_logs.groupby(\"user_id\").agg(\n",
    "    search_count=(\"search_query\", \"count\"),\n",
    "    unique_search_terms=(\"search_query\", pd.Series.nunique),\n",
    "    last_search_date=(\"search_date\", \"max\"),\n",
    ").reset_index()\n",
    "\n",
    "search_agg[\"days_since_last_search\"] = (SNAPSHOT_DATE - search_agg[\"last_search_date\"]).dt.days\n",
    "search_agg[\"days_since_last_search\"] = search_agg[\"days_since_last_search\"].replace({0: 1}).fillna(1)\n",
    "search_agg[\"search_frequency\"] = search_agg[\"search_count\"] / search_agg[\"days_since_last_search\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Recommendation engagement\n",
    "# -----------------------------\n",
    "rec_agg = recommendation_logs.groupby(\"user_id\").agg(\n",
    "    rec_impressions=(\"recommendation_id\", \"count\"),\n",
    "    rec_clicks=(\"was_clicked\", \"sum\"),\n",
    ").reset_index()\n",
    "\n",
    "rec_agg[\"rec_ctr\"] = rec_agg.apply(\n",
    "    lambda row: row[\"rec_clicks\"] / row[\"rec_impressions\"] if row[\"rec_impressions\"] else 0,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Sentiment / feedback\n",
    "# -----------------------------\n",
    "reviews[\"low_rating_flag\"] = (reviews[\"rating\"] <= 2).astype(int)\n",
    "\n",
    "review_agg = reviews.groupby(\"user_id\").agg(\n",
    "    avg_review_rating=(\"rating\", \"mean\"),\n",
    "    review_count=(\"rating\", \"count\"),\n",
    "    sentiment_score_avg=(\"sentiment_score\", \"mean\"),\n",
    "    complaint_count=(\"low_rating_flag\", \"sum\"),\n",
    ").reset_index()\n",
    "\n",
    "review_agg.fillna({\"avg_review_rating\": 0, \"sentiment_score_avg\": 0}, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Content affinity (genres, entropy, kids share)\n",
    "# -----------------------------\n",
    "watch_genre = watch_history.merge(\n",
    "    movies[[\"movie_id\", \"genre_primary\"]], on=\"movie_id\", how=\"left\"\n",
    ")\n",
    "watch_genre[\"genre_primary\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "genre_counts = watch_genre.groupby([\"user_id\", \"genre_primary\"]).size().reset_index(name=\"genre_count\")\n",
    "\n",
    "genre_totals = genre_counts.groupby(\"user_id\")[\"genre_count\"].sum()\n",
    "top_genre = genre_counts.groupby(\"user_id\")[\"genre_count\"].max()\n",
    "genre_diversity = genre_counts.groupby(\"user_id\")[\"genre_primary\"].nunique()\n",
    "\n",
    "def genre_entropy_fn(df):\n",
    "    counts = df[\"genre_count\"].values\n",
    "    if counts.sum() == 0:\n",
    "        return 0.0\n",
    "    probs = counts / counts.sum()\n",
    "    return float(-(probs * np.log2(probs)).sum())\n",
    "\n",
    "genre_entropy = genre_counts.groupby(\"user_id\").apply(genre_entropy_fn).rename(\"genre_entropy\")\n",
    "\n",
    "kids_genres = {\"Animation\", \"Family\", \"Kids\", \"Children\", \"Anime\"}\n",
    "watch_genre[\"is_kids_content\"] = watch_genre[\"genre_primary\"].isin(kids_genres).astype(int)\n",
    "kids_share = watch_genre.groupby(\"user_id\")[\"is_kids_content\"].mean().rename(\"kids_content_share\")\n",
    "\n",
    "genre_features = pd.concat(\n",
    "    [\n",
    "        genre_totals.rename(\"genre_total\"),\n",
    "        top_genre.rename(\"top_genre_count\"),\n",
    "        genre_diversity.rename(\"genre_diversity\"),\n",
    "        genre_entropy,\n",
    "        kids_share,\n",
    "    ],\n",
    "    axis=1,\n",
    ").reset_index()\n",
    "\n",
    "genre_features[\"top_genre_share\"] = genre_features[\"top_genre_count\"] / genre_features[\"genre_total\"]\n",
    "genre_features.fillna(0, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Merge all user-level feature frames\n",
    "# -----------------------------\n",
    "feature_frames = [watch_agg, search_agg, rec_agg, review_agg, genre_features]\n",
    "\n",
    "for frame in feature_frames:\n",
    "    user_features = user_features.merge(frame, on=\"user_id\", how=\"left\")\n",
    "\n",
    "numeric_cols_uf = user_features.select_dtypes(include=[np.number]).columns\n",
    "user_features[numeric_cols_uf] = user_features[numeric_cols_uf].fillna(0)\n",
    "\n",
    "categorical_cols_uf = user_features.select_dtypes(exclude=[np.number]).columns\n",
    "user_features[categorical_cols_uf] = user_features[categorical_cols_uf].fillna(\"Unknown\")\n",
    "\n",
    "print(\"user_features shape:\", user_features.shape)\n",
    "user_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333648c4-7e40-435c-a43a-2c957437bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Segmentation: numeric + one-hot categoricals\n",
    "# -----------------------------\n",
    "categorical_seg_cols = [\n",
    "    col for col in [\"subscription_plan\", \"primary_device\", \"country\"]\n",
    "    if col in user_features.columns\n",
    "]\n",
    "numeric_seg_cols = user_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "segmentation_base = user_features[[\"user_id\"] + numeric_seg_cols + categorical_seg_cols].copy()\n",
    "segmentation_base.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "if categorical_seg_cols:\n",
    "    segmentation_base[categorical_seg_cols] = segmentation_base[categorical_seg_cols].fillna(\"Unknown\")\n",
    "    categorical_dummies = pd.get_dummies(segmentation_base[categorical_seg_cols], prefix=categorical_seg_cols)\n",
    "else:\n",
    "    categorical_dummies = pd.DataFrame(index=segmentation_base.index)\n",
    "\n",
    "numeric_matrix = segmentation_base[numeric_seg_cols].fillna(0)\n",
    "\n",
    "segmentation_matrix = pd.concat([numeric_matrix, categorical_dummies], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "segmentation_scaled = scaler.fit_transform(segmentation_matrix)\n",
    "\n",
    "# -----------------------------\n",
    "# Choose k using silhouette (MiniBatchKMeans)\n",
    "# -----------------------------\n",
    "silhouette_scores = {}\n",
    "for k in range(2, 9):\n",
    "    mbk = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=256)\n",
    "    labels = mbk.fit_predict(segmentation_scaled)\n",
    "    silhouette_scores[k] = silhouette_score(segmentation_scaled, labels)\n",
    "\n",
    "best_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "cluster_model = MiniBatchKMeans(n_clusters=best_k, random_state=42, batch_size=256)\n",
    "cluster_labels = cluster_model.fit_predict(segmentation_scaled)\n",
    "\n",
    "segmentation_df = segmentation_base.copy()\n",
    "segmentation_df[\"cluster\"] = cluster_labels\n",
    "\n",
    "cluster_profiles = segmentation_df.groupby(\"cluster\")[numeric_seg_cols].mean()\n",
    "\n",
    "print(\"Silhouette scores:\", silhouette_scores)\n",
    "print(f\"Selected k = {best_k}\")\n",
    "cluster_profiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffcf99-0fa9-4e0e-a93a-0e950fbc0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Churn labeling anchored to SNAPSHOT_DATE\n",
    "# -----------------------------\n",
    "recent_window = SNAPSHOT_DATE - pd.Timedelta(days=28)\n",
    "prior_window = SNAPSHOT_DATE - pd.Timedelta(days=56)\n",
    "\n",
    "historical_watch = watch_history[watch_history[\"watch_date\"] <= SNAPSHOT_DATE].copy()\n",
    "future_watch = watch_history[watch_history[\"watch_date\"] > SNAPSHOT_DATE].copy()\n",
    "\n",
    "historical_search = search_logs[search_logs[\"search_date\"] <= SNAPSHOT_DATE].copy()\n",
    "future_search = search_logs[search_logs[\"search_date\"] > SNAPSHOT_DATE].copy()\n",
    "\n",
    "historical_recs = recommendation_logs[\n",
    "    recommendation_logs[\"recommendation_date\"] <= SNAPSHOT_DATE\n",
    "].copy()\n",
    "future_recs = recommendation_logs[\n",
    "    recommendation_logs[\"recommendation_date\"] > SNAPSHOT_DATE\n",
    "].copy()\n",
    "\n",
    "historical_reviews = reviews[reviews[\"review_date\"] <= SNAPSHOT_DATE].copy()\n",
    "future_reviews = reviews[reviews[\"review_date\"] > SNAPSHOT_DATE].copy()\n",
    "\n",
    "for df_split in (historical_watch, future_watch):\n",
    "    if \"is_download\" in df_split.columns:\n",
    "        df_split[\"is_download\"] = df_split[\"is_download\"].astype(int)\n",
    "\n",
    "if \"was_clicked\" in historical_recs.columns:\n",
    "    historical_recs[\"was_clicked\"] = historical_recs[\"was_clicked\"].astype(int)\n",
    "if \"was_clicked\" in future_recs.columns:\n",
    "    future_recs[\"was_clicked\"] = future_recs[\"was_clicked\"].astype(int)\n",
    "\n",
    "all_users = users[\"user_id\"].unique()\n",
    "\n",
    "# -----------------------------\n",
    "# Historical features (for modeling)\n",
    "# -----------------------------\n",
    "watch_stats = historical_watch.groupby(\"user_id\").agg(\n",
    "    total_watch_minutes=(\"watch_duration_minutes\", \"sum\"),\n",
    "    avg_completion=(\"progress_percentage\", \"mean\"),\n",
    "    unique_devices=(\"device_type\", \"nunique\"),\n",
    "    completed_ratio=(\"action\", lambda x: (x == \"completed\").mean()),\n",
    ").reset_index()\n",
    "\n",
    "for col in [\"total_watch_minutes\", \"avg_completion\", \"unique_devices\", \"completed_ratio\"]:\n",
    "    watch_stats[col].fillna(0, inplace=True)\n",
    "\n",
    "download_ratio = historical_watch.groupby(\"user_id\")[\"is_download\"].mean().reset_index(\n",
    "    name=\"download_ratio\"\n",
    ")\n",
    "watch_stats = watch_stats.merge(download_ratio, on=\"user_id\", how=\"left\")\n",
    "watch_stats[\"download_ratio\"].fillna(0, inplace=True)\n",
    "\n",
    "sessions_last_28 = historical_watch[historical_watch[\"watch_date\"] >= recent_window]\n",
    "sessions_prev_28 = historical_watch[\n",
    "    (historical_watch[\"watch_date\"] >= prior_window)\n",
    "    & (historical_watch[\"watch_date\"] < recent_window)\n",
    "]\n",
    "\n",
    "last_28_counts = sessions_last_28.groupby(\"user_id\").size().reset_index(\n",
    "    name=\"sessions_last_28_days\"\n",
    ")\n",
    "prev_28_counts = sessions_prev_28.groupby(\"user_id\").size().reset_index(\n",
    "    name=\"sessions_prev_28_days\"\n",
    ")\n",
    "\n",
    "watch_stats = watch_stats.merge(last_28_counts, on=\"user_id\", how=\"left\")\n",
    "watch_stats = watch_stats.merge(prev_28_counts, on=\"user_id\", how=\"left\")\n",
    "watch_stats[[\"sessions_last_28_days\", \"sessions_prev_28_days\"]] = watch_stats[\n",
    "    [\"sessions_last_28_days\", \"sessions_prev_28_days\"]\n",
    "].fillna(0)\n",
    "watch_stats[\"session_trend_4w\"] = (\n",
    "    watch_stats[\"sessions_last_28_days\"] - watch_stats[\"sessions_prev_28_days\"]\n",
    ")\n",
    "\n",
    "historical_sessions = historical_watch.groupby(\"user_id\").size().rename(\"historical_sessions\")\n",
    "\n",
    "search_stats = historical_search.groupby(\"user_id\").size().reset_index(\n",
    "    name=\"historical_searches\"\n",
    ")\n",
    "\n",
    "rec_stats = historical_recs.groupby(\"user_id\").agg(\n",
    "    recs_shown=(\"recommendation_id\", \"count\"),\n",
    "    rec_clicks=(\"was_clicked\", \"sum\"),\n",
    ").reset_index()\n",
    "rec_stats[\"rec_ctr\"] = rec_stats[\"rec_clicks\"].div(\n",
    "    rec_stats[\"recs_shown\"].replace(0, np.nan)\n",
    ").fillna(0)\n",
    "\n",
    "review_stats = historical_reviews.groupby(\"user_id\").agg(\n",
    "    avg_review_rating=(\"rating\", \"mean\"),\n",
    "    review_count=(\"rating\", \"count\"),\n",
    "    low_rating_ratio=(\"rating\", lambda x: (x <= 2).mean()),\n",
    "    helpful_votes_avg=(\"helpful_votes\", \"mean\"),\n",
    "    sentiment_score_avg=(\"sentiment_score\", \"mean\"),\n",
    ").reset_index()\n",
    "review_stats.fillna(0, inplace=True)\n",
    "\n",
    "watch_with_genre = historical_watch.merge(\n",
    "    movies[[\"movie_id\", \"genre_primary\"]], on=\"movie_id\", how=\"left\"\n",
    ")\n",
    "genre_counts_hist = watch_with_genre.groupby([\"user_id\", \"genre_primary\"]).size().reset_index(\n",
    "    name=\"genre_count\"\n",
    ")\n",
    "genre_totals_hist = genre_counts_hist.groupby(\"user_id\")[\"genre_count\"].sum().rename(\"genre_total\")\n",
    "genre_max_hist = genre_counts_hist.groupby(\"user_id\")[\"genre_count\"].max().rename(\n",
    "    \"top_genre_count\"\n",
    ")\n",
    "genre_div_hist = watch_with_genre.groupby(\"user_id\")[\"genre_primary\"].nunique().rename(\n",
    "    \"genre_diversity\"\n",
    ")\n",
    "genre_features_hist = pd.concat(\n",
    "    [genre_totals_hist, genre_max_hist, genre_div_hist], axis=1\n",
    ")\n",
    "genre_features_hist[\"top_genre_share\"] = (\n",
    "    genre_features_hist[\"top_genre_count\"] / genre_features_hist[\"genre_total\"]\n",
    ")\n",
    "genre_features_hist = genre_features_hist.reset_index().fillna(0)\n",
    "\n",
    "plan_map_churn = {\"Basic\": 0, \"Standard\": 1, \"Premium\": 2, \"Premium+\": 3}\n",
    "users_financial = users[\n",
    "    [\n",
    "        \"user_id\",\n",
    "        \"subscription_plan\",\n",
    "        \"primary_device\",\n",
    "        \"monthly_spend\",\n",
    "        \"household_size\",\n",
    "        \"subscription_start_date\",\n",
    "        \"is_active\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "users_financial[\"tenure_days\"] = (SNAPSHOT_DATE - users_financial[\"subscription_start_date\"]).dt.days\n",
    "users_financial[\"plan_code\"] = users_financial[\"subscription_plan\"].map(plan_map_churn).fillna(0)\n",
    "users_financial[\"primary_device_code\"] = (\n",
    "    users_financial[\"primary_device\"].astype(\"category\").cat.codes\n",
    ")\n",
    "\n",
    "if users_financial[\"is_active\"].dtype == \"O\":\n",
    "    users_financial[\"is_active_flag\"] = users_financial[\"is_active\"].astype(str).str.lower().isin(\n",
    "        [\"true\", \"1\", \"yes\"]\n",
    "    )\n",
    "else:\n",
    "    users_financial[\"is_active_flag\"] = users_financial[\"is_active\"].astype(bool)\n",
    "\n",
    "users_financial[\"is_active_flag\"] = users_financial[\"is_active_flag\"].astype(int)\n",
    "\n",
    "users_financial.drop(\n",
    "    columns=[\"subscription_plan\", \"primary_device\", \"subscription_start_date\", \"is_active\"],\n",
    "    inplace=True,\n",
    ")\n",
    "users_financial[[\"monthly_spend\", \"household_size\", \"tenure_days\"]] = users_financial[\n",
    "    [\"monthly_spend\", \"household_size\", \"tenure_days\"]\n",
    "].fillna(0)\n",
    "\n",
    "feature_frames_churn = [watch_stats, search_stats, rec_stats, review_stats, genre_features_hist]\n",
    "\n",
    "user_activity_churn = users_financial.copy()\n",
    "for frame in feature_frames_churn:\n",
    "    user_activity_churn = user_activity_churn.merge(frame, on=\"user_id\", how=\"left\")\n",
    "\n",
    "numeric_cols_churn = user_activity_churn.select_dtypes(include=[\"number\"]).columns\n",
    "user_activity_churn[numeric_cols_churn] = user_activity_churn[numeric_cols_churn].fillna(0)\n",
    "\n",
    "user_activity_churn = user_activity_churn.merge(\n",
    "    historical_sessions.reset_index(), on=\"user_id\", how=\"left\"\n",
    ")\n",
    "user_activity_churn[\"historical_sessions\"].fillna(0, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Future engagement for churn label\n",
    "# -----------------------------\n",
    "future_df = pd.DataFrame(index=all_users)\n",
    "future_df[\"future_watch_events\"] = future_watch.groupby(\"user_id\").size().reindex(\n",
    "    all_users, fill_value=0\n",
    ")\n",
    "future_df[\"future_search_events\"] = future_search.groupby(\"user_id\").size().reindex(\n",
    "    all_users, fill_value=0\n",
    ")\n",
    "if \"was_clicked\" in future_recs.columns:\n",
    "    future_clicks = future_recs[future_recs[\"was_clicked\"] == 1].groupby(\"user_id\").size()\n",
    "else:\n",
    "    future_clicks = pd.Series(dtype=int)\n",
    "\n",
    "future_df[\"future_rec_clicks\"] = future_clicks.reindex(all_users, fill_value=0)\n",
    "future_df[\"future_downloads\"] = future_watch.groupby(\"user_id\")[\"is_download\"].sum().reindex(\n",
    "    all_users, fill_value=0\n",
    ")\n",
    "future_df[\"future_reviews\"] = future_reviews.groupby(\"user_id\").size().reindex(\n",
    "    all_users, fill_value=0\n",
    ")\n",
    "\n",
    "user_status = users.drop_duplicates(\"user_id\", keep=\"last\").set_index(\"user_id\")\n",
    "status = user_status[\"is_active\"].copy()\n",
    "if status.dtype == \"O\":\n",
    "    status = status.astype(str).str.lower().isin([\"true\", \"1\", \"yes\"])\n",
    "else:\n",
    "    status = status.astype(bool)\n",
    "\n",
    "mon_spend = user_status[\"monthly_spend\"].fillna(0)\n",
    "\n",
    "future_df[\"inactive_or_no_spend\"] = (\n",
    "    ~status.reindex(all_users, fill_value=False)\n",
    ") | (mon_spend.reindex(all_users, fill_value=0) <= 0)\n",
    "\n",
    "future_df = future_df.fillna(0)\n",
    "\n",
    "future_df[\"churn\"] = (\n",
    "    (future_df[\"future_watch_events\"] == 0)\n",
    "    & (future_df[\"future_search_events\"] == 0)\n",
    "    & (future_df[\"future_rec_clicks\"] == 0)\n",
    "    & (future_df[\"future_downloads\"] == 0)\n",
    "    & (future_df[\"future_reviews\"] == 0)\n",
    ").astype(int)\n",
    "\n",
    "user_activity_churn = user_activity_churn.merge(\n",
    "    future_df[[\"churn\"]], left_on=\"user_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "user_activity_churn[\"churn\"].fillna(0, inplace=True)\n",
    "\n",
    "# Require at least 2 historical sessions to be labelable\n",
    "user_activity_churn = user_activity_churn[user_activity_churn[\"historical_sessions\"] > 1].copy()\n",
    "\n",
    "print(\"user_activity_churn shape:\", user_activity_churn.shape)\n",
    "user_activity_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b9675-040f-41ec-ba8e-3f65fe02e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_cols = [c for c in user_activity_churn.columns if c.startswith(\"future_\")]\n",
    "leakage_cols = [\n",
    "    \"inactive_or_no_spend\",\n",
    "    \"recency_days\",\n",
    "    \"interaction_count\",\n",
    "    \"inactivity_days\",\n",
    "    \"total_watch_time\",\n",
    "    \"avg_rating\",\n",
    "    \"genre_primary\",\n",
    "]\n",
    "\n",
    "exclude_cols = set(future_cols + leakage_cols + [\"churn\"])\n",
    "\n",
    "numeric_cols = (\n",
    "    user_activity_churn.drop(columns=list(exclude_cols), errors=\"ignore\")\n",
    "    .select_dtypes(include=np.number)\n",
    "    .columns.tolist()\n",
    ")\n",
    "\n",
    "X_all = user_activity_churn[numeric_cols].copy()\n",
    "churn_features = pd.concat([X_all, user_activity_churn[\"churn\"]], axis=1)\n",
    "churn_features = churn_features.loc[:, ~churn_features.columns.duplicated()]\n",
    "\n",
    "print(f\"Using {len(numeric_cols)} numeric historical features for churn modeling.\")\n",
    "print(\"Sample feature list:\", numeric_cols[:10])\n",
    "print(\"Churn distribution (counts):\")\n",
    "print(user_activity_churn[\"churn\"].value_counts())\n",
    "\n",
    "corr_with_churn = (\n",
    "    churn_features.corr(numeric_only=True)[\"churn\"]\n",
    "    .drop(\"churn\")\n",
    "    .sort_values(key=lambda s: s.abs(), ascending=False)\n",
    ")\n",
    "print(\"Top correlated features with churn (abs sorted):\")\n",
    "print(corr_with_churn.head(10))\n",
    "\n",
    "churn_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f21838-35b6-45dc-b2d1-7b2228403748",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn_features.drop(\"churn\", axis=1)\n",
    "y = churn_features[\"churn\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train class distribution:\\n\", y_train.value_counts())\n",
    "print(\"Test class distribution:\\n\", y_test.value_counts())\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "majority = train_df[train_df[\"churn\"] == 0]\n",
    "minority = train_df[train_df[\"churn\"] == 1]\n",
    "\n",
    "minority_upsampled = resample(\n",
    "    minority,\n",
    "    replace=True,\n",
    "    n_samples=len(majority),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_balanced = pd.concat([majority, minority_upsampled])\n",
    "X_train_bal = train_balanced.drop(\"churn\", axis=1)\n",
    "y_train_bal = train_balanced[\"churn\"].astype(int)\n",
    "\n",
    "print(\"After upsampling:\\n\", y_train_bal.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b6d6a-65c0-4138-b445-3bbe5cb04363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic baseline\n",
    "log_model = LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    "log_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Class imbalance handling\n",
    "scale_pos_weight = max(((y_train == 0).sum() / max((y_train == 1).sum(), 1)), 1)\n",
    "class_weight_dict = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    random_state=42,\n",
    ")\n",
    "rf_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# HistGradientBoosting\n",
    "hgb_model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=20,\n",
    "    max_iter=400,\n",
    "    random_state=42,\n",
    ")\n",
    "hgb_sample_weight = np.where(y_train == 1, scale_pos_weight, 1.0)\n",
    "hgb_model.fit(X_train, y_train, sample_weight=hgb_sample_weight)\n",
    "\n",
    "# XGBoost (optional)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        eval_metric=\"logloss\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=500,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "except ImportError:\n",
    "    xgb_model = None\n",
    "\n",
    "# LightGBM (optional)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "except ImportError:\n",
    "    lgb_model = None\n",
    "\n",
    "# Calibrated SGD (logistic-like)\n",
    "sgd_base = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    penalty=\"l2\",\n",
    "    alpha=1e-4,\n",
    "    max_iter=2000,\n",
    "    tol=1e-3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    ")\n",
    "calibrated_sgd = CalibratedClassifierCV(sgd_base, method=\"sigmoid\", cv=3)\n",
    "calibrated_sgd.fit(X_train, y_train)\n",
    "\n",
    "model_registry = {\n",
    "    \"logistic_default\": log_model,\n",
    "    \"random_forest\": rf_model,\n",
    "    \"hist_gradient_boosting\": hgb_model,\n",
    "    \"xgboost\": xgb_model,\n",
    "    \"lightgbm\": lgb_model,\n",
    "    \"calibrated_sgd\": calibrated_sgd,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0c9e8-2cde-476a-87c7-61012113ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(label, y_true, y_pred, y_proba):\n",
    "    print(f\"\\n{label} Results:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_true, y_proba):.3f}\")\n",
    "    print(f\"PR-AUC: {average_precision_score(y_true, y_proba):.3f}\")\n",
    "\n",
    "# Logistic baseline with threshold tuning\n",
    "y_proba_log = log_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_log)\n",
    "beta = 2.0\n",
    "beta_sq = beta ** 2\n",
    "fbeta_scores = ((1 + beta_sq) * precisions[:-1] * recalls[:-1]) / np.where(\n",
    "    (beta_sq * precisions[:-1] + recalls[:-1]) == 0,\n",
    "    np.nan,\n",
    "    (beta_sq * precisions[:-1] + recalls[:-1]),\n",
    ")\n",
    "best_idx = np.nanargmax(fbeta_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "y_pred_log_default = (y_proba_log >= 0.5).astype(int)\n",
    "y_pred_log_tuned = (y_proba_log >= best_threshold).astype(int)\n",
    "\n",
    "print_metrics(\"Logistic Regression (0.5 threshold)\", y_test, y_pred_log_default, y_proba_log)\n",
    "print(f\"Best logistic threshold (F2-max): {best_threshold:.3f} with F2={fbeta_scores[best_idx]:.3f}\")\n",
    "print_metrics(\"Logistic Regression (F2 tuned)\", y_test, y_pred_log_tuned, y_proba_log)\n",
    "\n",
    "# Other models at 0.5 threshold\n",
    "for name, model in model_registry.items():\n",
    "    if model is None or name == \"logistic_default\":\n",
    "        continue\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        decision = model.decision_function(X_test)\n",
    "        proba = 1 / (1 + np.exp(-decision))\n",
    "    else:\n",
    "        proba = model.predict(X_test)\n",
    "\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    pretty_name = name.replace(\"_\", \" \").title()\n",
    "    print_metrics(pretty_name, y_test, preds, proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea1bce-c498-47eb-91ea-469d0cb5db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLV-weighted churn scoring at user level ===\n",
    "\n",
    "candidate_order = [\n",
    "    \"hist_gradient_boosting\",\n",
    "    \"lightgbm\",\n",
    "    \"xgboost\",\n",
    "    \"random_forest\",\n",
    "    \"logistic_default\",\n",
    "    \"calibrated_sgd\",\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "for name in candidate_order:\n",
    "    model = model_registry.get(name)\n",
    "    if model is not None:\n",
    "        best_model_name = name\n",
    "        best_model = model\n",
    "        break\n",
    "\n",
    "if best_model is None:\n",
    "    raise RuntimeError(\"No available churn model found in model_registry for CLV-weighted scoring.\")\n",
    "\n",
    "print(\"Using model for CLV-weighted churn scoring:\", best_model_name)\n",
    "\n",
    "X_full = user_activity_churn[numeric_cols].copy()\n",
    "\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    churn_proba_all = best_model.predict_proba(X_full)[:, 1]\n",
    "elif hasattr(best_model, \"decision_function\"):\n",
    "    decision = best_model.decision_function(X_full)\n",
    "    churn_proba_all = 1.0 / (1.0 + np.exp(-decision))\n",
    "else:\n",
    "    churn_proba_all = best_model.predict(X_full)\n",
    "\n",
    "churn_scored = user_activity_churn[[\"user_id\", \"churn\"]].copy()\n",
    "churn_scored[\"churn_proba\"] = churn_proba_all\n",
    "\n",
    "print(\"churn_scored shape:\", churn_scored.shape)\n",
    "churn_scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d4108-bb00-4eba-86d6-4ad516b088e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Attach CLV features and compute expected value loss ===\n",
    "\n",
    "financial_cols = [\n",
    "    \"cumulative_revenue\",\n",
    "    \"clv_baseline\",\n",
    "    \"tenure_days\",\n",
    "    \"months_active\",\n",
    "    \"monthly_spend\",\n",
    "]\n",
    "financial_cols_present = [c for c in financial_cols if c in user_features.columns]\n",
    "\n",
    "clv_df = user_features[[\"user_id\"] + financial_cols_present].copy()\n",
    "\n",
    "churn_scored = churn_scored.merge(clv_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "if \"clv_baseline\" not in churn_scored.columns:\n",
    "    churn_scored[\"clv_baseline\"] = np.nan\n",
    "\n",
    "churn_scored[\"clv_baseline\"] = churn_scored[\"clv_baseline\"].fillna(\n",
    "    churn_scored.get(\"cumulative_revenue\")\n",
    ")\n",
    "\n",
    "if \"months_active\" in churn_scored.columns:\n",
    "    approx_clv = churn_scored.get(\"monthly_spend\", 0) * churn_scored[\"months_active\"].fillna(0)\n",
    "else:\n",
    "    approx_clv = churn_scored.get(\"monthly_spend\", 0) * (\n",
    "        churn_scored.get(\"tenure_days\", 0) / 30.0\n",
    "    )\n",
    "\n",
    "churn_scored[\"clv_baseline\"] = churn_scored[\"clv_baseline\"].fillna(approx_clv)\n",
    "churn_scored[\"clv_baseline\"] = churn_scored[\"clv_baseline\"].fillna(0)\n",
    "\n",
    "churn_scored[\"expected_value_loss\"] = churn_scored[\"clv_baseline\"] * churn_scored[\"churn_proba\"]\n",
    "\n",
    "churn_scored[\"clv_band\"] = pd.qcut(\n",
    "    churn_scored[\"clv_baseline\"].replace(0, np.nan).fillna(0),\n",
    "    q=3,\n",
    "    labels=[\"low\", \"medium\", \"high\"],\n",
    "    duplicates=\"drop\",\n",
    ")\n",
    "churn_scored[\"churn_risk_band\"] = pd.qcut(\n",
    "    churn_scored[\"churn_proba\"],\n",
    "    q=3,\n",
    "    labels=[\"low\", \"medium\", \"high\"],\n",
    "    duplicates=\"drop\",\n",
    ")\n",
    "\n",
    "churn_scored[\"priority_segment\"] = np.where(\n",
    "    (churn_scored[\"clv_band\"] == \"high\") & (churn_scored[\"churn_risk_band\"] == \"high\"),\n",
    "    \"High CLV / High Churn\",\n",
    "    \"Other\",\n",
    ")\n",
    "\n",
    "print(\"CLV-weighted churn frame shape:\", churn_scored.shape)\n",
    "churn_scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb3d0c-8e30-4de9-b3bf-f29daad239bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Join churn + segmentation: cluster-wise churn & value loss ===\n",
    "\n",
    "print(\"churn_scored columns:\", list(churn_scored.columns))\n",
    "\n",
    "segmentation_with_id = segmentation_df.copy()\n",
    "if segmentation_with_id.index.name == \"user_id\":\n",
    "    segmentation_with_id = segmentation_with_id.reset_index()\n",
    "\n",
    "desired_join_cols = [\n",
    "    \"user_id\",\n",
    "    \"churn\",\n",
    "    \"churn_proba\",\n",
    "    \"clv_baseline\",\n",
    "    \"expected_value_loss\",\n",
    "    \"clv_band\",\n",
    "    \"churn_risk_band\",\n",
    "    \"priority_segment\",\n",
    "]\n",
    "available_join_cols = [c for c in desired_join_cols if c in churn_scored.columns]\n",
    "print(\"Using join columns from churn_scored:\", available_join_cols)\n",
    "\n",
    "segmentation_enriched = segmentation_with_id.merge(\n",
    "    churn_scored[available_join_cols],\n",
    "    on=\"user_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "agg_dict = {\n",
    "    \"users_in_cluster\": (\"user_id\", \"nunique\"),\n",
    "    \"churn_rate\": (\"churn\", \"mean\"),\n",
    "    \"avg_churn_proba\": (\"churn_proba\", \"mean\"),\n",
    "    \"avg_clv\": (\"clv_baseline\", \"mean\"),\n",
    "}\n",
    "\n",
    "if \"expected_value_loss\" in segmentation_enriched.columns:\n",
    "    agg_dict[\"total_expected_value_loss\"] = (\"expected_value_loss\", \"sum\")\n",
    "\n",
    "cluster_churn_summary = segmentation_enriched.groupby(\"cluster\").agg(**agg_dict).reset_index()\n",
    "\n",
    "print(\"Cluster-level churn & CLV summary:\")\n",
    "cluster_churn_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644d271-877f-4a82-96c4-f159c62f3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PCA & visualizations on enriched segmentation matrix ===\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "segmentation_pca = pca.fit_transform(segmentation_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    segmentation_pca,\n",
    "    columns=[\"pc1\", \"pc2\"],\n",
    "    index=segmentation_matrix.index,\n",
    ")\n",
    "\n",
    "pca_df = pca_df.join(segmentation_df[[\"cluster\"]], how=\"left\")\n",
    "pca_df = pca_df.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "\n",
    "pca_df = pca_df.merge(\n",
    "    churn_scored[[\"user_id\", \"churn_proba\", \"expected_value_loss\", \"clv_baseline\"]],\n",
    "    on=\"user_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    pca_df[\"pc1\"],\n",
    "    pca_df[\"pc2\"],\n",
    "    c=pca_df[\"cluster\"],\n",
    "    cmap=\"tab10\",\n",
    "    alpha=0.6,\n",
    "    s=20,\n",
    ")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Customer Segments in PCA Space (colored by cluster)\")\n",
    "plt.colorbar(scatter, label=\"cluster\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "clv_risk = pca_df[\"expected_value_loss\"].fillna(0)\n",
    "scatter2 = plt.scatter(\n",
    "    pca_df[\"pc1\"],\n",
    "    pca_df[\"pc2\"],\n",
    "    c=clv_risk,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.6,\n",
    "    s=20,\n",
    ")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Map Colored by Expected Value Loss\")\n",
    "plt.colorbar(scatter2, label=\"expected_value_loss\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance by PCs:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f7418-29a3-4c01-8f3e-fa48c7a06bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
